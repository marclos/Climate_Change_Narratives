\documentclass{article}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{gensymb}
\usepackage{hyperref}
\hypersetup{%
  colorlinks = true,
  linkcolor  = black
}

\title{Evaluating Monthly Trends using CHCN-Daily}
\author{Marc Los Huertos}

% Setting up the margins, etc for R
<<echo = F, results='hide'>>=
options(width=50)
rm(list = ls())
@

<<echo=FALSE>>=
# Importing Scripts
source("/home/CAMPUS/mwl04747/github/Climate_Change_Narratives/Custom_Code/CSV_import-fun.R")
climate_data <- CSV_import("/home/CAMPUS/mwl04747/github/Climate_Change_Narratives/Data/MLH/Thailand_GHCND:TH000048456.csv")
@

\begin{document}

\maketitle
\tableofcontents

\begin{abstract}
\noindent This SOP will provide the tools to analyze the CDO climate data for long-term trends in the daily data. We will also create monthly averages and determine if some months have a stronger trend than others.   

\end{abstract}

\section{Introduction}

\subsection{Goals for This Document}

This document provides EA students with the methods to analyze climate data based on monthly averages and evaluate if these data are reliable compared to the CHCN-Monthly and investigate sources of uncertainty. 

\section{Creating a Best Fit Line for Daily TMAX data}

\subsection{To Begin}

You should have R code that generates a plot of daily TMAX data for a site with a best fit line overlaid. If not, please go back to ``Using NOAA Clamate Records.''

\subsection{Generalized Steps}

In this SOP you will\ldots

\begin{enumerate}
  \item plot the temperature vs. date and overlay a bestfit line
  \item create new variables for date and month;
  \item create a new dataframe with monthly averages;
  \item model and estimate average trend for each month;
  \item evaluate the validity of the models; and
  \item interpret the trend data
\end{enumerate}

\subsection{Regression and Climate Change}

One of the ourcomes of the linear regression is to estimate the best fit line

\begin{equation}
y = mx + b + \epsilon,
\end{equation}

where $\epsilon$ is an estimate of the error. In addition, two other estimates are provided, one for the slope, $m$, and the y-intercept, $b$. 

But these estimates are also hypotheses, where the null hypothesis is:

\begin{description}
  \item[slope is zero] Rejecting the null hypothesis would be support the alternative hypothesis, or the estimate of the slope. 
  \item[y-intercept is zero] Rejecting the null hypothesis would support the alternative hypothesis, the estimate of the y-intercept.
\end{description}

Okay, let's see if we can do this for our Bangkok data. Let's test if there is a significant change of daily maximum temperatures (TMAX) with time. Thus, in general terms, Maximum temperature is a function of time, or $TMAX = f(Time)$. 

\begin{equation}
TMAX \sim \alpha + \beta * time + \epsilon
\end{equation}

Translating this in R will take some additional tricks besides just getting the code figured out. First, we need to identify the predictor variable, 'NewDate', in the data frame which we created in "2 Using NOAA Climate Records". 

For the response variable, we will use the daily maximum temeratures, TMAX. Remember there are some missing data, it will be interesting to note how R deals with that.

\subsection{Creating a plot and linear model}

First, let's create a plot of data using \texttt{plot()}, whose format is \texttt{plot(x, y)} or \texttt{plot(y $\sim$ x)}. We will use the later for now, e.g. plot(TMAX $\sim$ NewDate, data=climate\_data) (Figure \ref{fig:test12}).

\begin{figure}
\caption{Maximum daily temperatures for Bangkok, Thailand.}
\label{fig:test12}
<<label='Tmaxplot', echo = F, fig = 'true' >>=
par(las=1)
plot(TMAX ~ NewDate, data=climate_data, ylab="degrees C", xlab="Date", pch=20, main="TMAX, \n Bankok, Thailand" )
@
\end{figure}

We use the \texttt{lm()} function that arrange the results in-line with a regression model. This syntax is straight forward,  

<<label='linearmodel'>>=
lm(TMAX ~ NewDate, data=climate_data)
@

\noindent Notice that we had define the data source as we created the linearm model. In addition, the format is nearly identical to the \texttt{plot()} function. That's convenient!

From this model, we learn that the change in $TMAX$ is 
\Sexpr{round(coef(lm(TMAX ~ NewDate, data=climate_data))[2], 6)}~\degree~C~year$^{-1}$. Figure~\ref{fig:TMAX_trend} shows a trend of increasing maximum temperatures.

We know tha the slope and intercept can used to define a line -- so, next we can add a line to the plot. First, let's see how we can create a linear model object using \texttt{lm() }and extract the coefficients from linear model, usinig \texttt{coef()}.

I am going to name the linear model with an ``.lm'' suffix to keep track of the type of object created. Then name an object ``slopeintercept'' to be clear where those two values lie. 

<<>>=
TMAX.lm = lm(TMAX ~ NewDate, data= climate_data)
slopeintercept = coef(TMAX.lm)
@

\noindent Now, we can plot the the data with the best fit line. 

<<eval=FALSE>>=
plot(TMAX ~ NewDate, data= climate_data, las=1)
abline(slopeintercept, col="red", lwd=3)
@

\noindent Notice that I added some arguments in the function to improve the graphics. However, I added others in Figure~\ref{fig:TMAX_trend}, like x- and y-axis labels, that I am not showing to keep coding easier. I encourage you to look up these other arguments by searching online. 

% Additional LaTeX code to add caption to figure
\begin{figure}
\caption{Maximum Daily Temperatures in Bangkok, Thailand.}
\label{fig:TMAX_trend}
<<echo = F, fig = 'true' >>=
par(las=1)
plot(TMAX ~ NewDate, data=climate_data, type="l", ylab="degrees", xlab="Date", main="Maximum Daily Temperature, \n Bangkok, Thailand", las=1 )

abline(coef(lm(TMAX~NewDate, data=climate_data)), col="red", lwd=2)
@
\end{figure}

\subsection{Interpreting the Results}

Now determine test the null hypotheses and use the \texttt{summary()} function to display many of the important regression results.

<<label='summarylm_TMAX'>>=
(TMAX.lm.sum = summary(lm(TMAX ~ NewDate, data=climate_data)))
@

<<echo=FALSE>>=
lmp <- function (modelobject) {
    if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
    f <- summary(modelobject)$fstatistic
    p <- pf(f[1],f[2],f[3],lower.tail=F)
    attributes(p) <- NULL
    return(p)
}

# lmp(TMAX.lm)
@

Based on the results, we reject the null hypotheses, i.e. the events that this might occur by chance is small:  $<$ 2x10$^{-16}$ for the slope is zero and p $<$ 2x10$^{-16}$ for the y-intercept is zero.

Because these data are in a time series, they are serially correlated, meaning that the June sample will be more like the July sample than the August sample. In addition, the June 2010 sample will be similar to the June 2009 sample. These correlations violate the assumption of independence, but for now, we will ignore this violation and just create a linear model in bliss.

In addition, we have some information on the residuals and r$^2$ estimates, which are important to interpret the model -- basically how much of the variation is explained by x (date). In our case, we obtain \Sexpr{round(TMAX.lm.sum$r.squared*100,2)}\%, which is pretty low, generally, we shouldn't get excited about a regression unless we approach 30-40\%! At some point, we will sort out why the r$^2$ is so low, but for now, keep moving. 

For now, we can appreciate the the temperature is changing, i.e. increasing, with a slope of \Sexpr{coef(TMAX.lm)[2]}\degree C per year or if we multiply by 100 to make it more ``relateable'', we obtain a change of \Sexpr{round(coef(TMAX.lm)[2]*100,4)}\degree C/100 years--still pretty low! It begs the question, how important is climate change in Bangkok?

\section{Aggregating Data into Monthly Means}

\subsection{Monthly Means of Daily Maximum Temperatures}

One of the first things to note is how messy the data look and there are lots of sources of variation. For example, we expect months to respond differently to the climate change. To assess this, we will now analyze the data for monthly means of the maximum temperatures.

\subsection{Creating Monthly Means}

To create monthly means, we need to disagragate the NewDate variable into a month and year variables.

First we can use the \texttt{as.Date()} function to extract a portion of the date, where \%m is for month and \%Y is for a four digit year. Then, we create new variables in our dataframe, one for month and one for year.

<<label='createmonthvar'>>=
climate_data$Month = format(as.Date(climate_data$NewDate), format = "%m")
climate_data$Year = format(climate_data$NewDate, format="%Y")
@

Note: 
Some data sets might not show the century in front of the year, which confuses R when plotting graphs, since it cannot distinguish between different centuries. For example, if under Year in your data set you only see “10”,”11”,”12” and your data runs from 1910 to 2010, R will not know how to distinguish the “10” from 1910 and the “10” from 2010, and your plot will jump back and forward a century. Therefore, you need to format your years so that it displays the century. This is how it is done:

1) Open up your climate\_data table from your files in R studio.

2) Find the point at which the entries jump from one century to the next. (eg. 1999 to 2000)

3) Write down the row number of the last entry of the earlier century (1999) and the first entry of the new century (2000). 

4) Format your code in R using [1:(last entry of 19th century)], [(first entry of 19th century):(last entry)]

<<label='Addcenturies'>>=
climate_data$NewDate[1:23344] = format(as.Date(strDates[1:23344], "%m/%d/%y"), "19%y-%m-%d") 
climate_data$NewDate[23345:45332] = format(as.Date(strDates[23345:45332], "%m/%d/%y"),"20%y-%m-%d")
@


After creating the month and year as separate variables, we can use them to caculate the mean using the \texttt{aggregate()} function. In the code below, we can also calculate the standard deviation too, although I haven't used this measure in this document, several students have asked for this for their analysis.

<<label='MonthlyTMAXmean'>>=
MonthlyTMAXMean = aggregate(TMAX ~ Month + Year, climate_data, mean)
@

Note: I always check to see what I have created to make sure I am getting what I expected!  

For example, when we look at the structure of the MonthlyTMAXMean, I learn that the `Year' and `Month' format -- which are defined as character strings.

<<>>=
str(MonthlyTMAXMean)
@

We need to change this to numeric formats so we can further process the data. 

<<>>=
MonthlyTMAXMean$Year.num = as.numeric(MonthlyTMAXMean$Year)
MonthlyTMAXMean$Month.num = as.numeric(MonthlyTMAXMean$Month)
@

\noindent And checking the data\ldots
<<label='MonthlyTMAXmeanCheck'>>=
str(MonthlyTMAXMean)
@
\noindent It worked!

Now, let's see what plot looks like. Should look pretty different than the daily data we had before. 
<<>>=
plot(MonthlyTMAXMean$TMAX, ty='l')
@

Reminder: When we define a plot type, (ty = ), we are defining it as a line which is abreviated as the letter `l' and abrevation for `line', not the number '1'. Altenatively, you could make a ``point'' plot with ty = `p'.  

\subsection{Selecting for 1 Month -- May}

Perhaps, we can get a better handle on this stuff if we analyze for just one month at a time -- certainly easier to visualize!

Note: I have limited the data limits to evaluate the a subset of the years from 1950--2020. Also, I can use the numbers because the Year.num is a numeric variable.

In addition, I have selected the 5th month of the year, i.e. May!  Note that I am referencing the variable that is a character string, thus need to put ``05'' in quotes. 

<<>>=
plot(TMAX~Year.num, data=MonthlyTMAXMean[MonthlyTMAXMean$Month=="05",],
     ty='l', xlim=c(1950, 2020)) 
May.lm <- lm(TMAX~Year.num, data=MonthlyTMAXMean[MonthlyTMAXMean$Month=="05",])
summary(May.lm)

abline(coef(May.lm), col="red")
@

Now, the change is \Sexpr{round(coef(May.lm)[2], 4)} degress C/year or \Sexpr{round(coef(May.lm)[2]*100, 3)} degress C/100 years with a probability of \Sexpr{round(summary(May.lm)$coefficients[2,4],4)}. Although we can't reject the null hypothesis, we find the method to be fairly straightforward! 

%https://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R

Note: if you are getting an error saying: error in plot.window(need finite `ylim' values), you may need to check your data format and see whether the months in your data set have a 0 infront of them or not. If not, try ``5" instead of ``05".

\subsection{Testing all the Months}

This section for advanced users and specific questions. Please do not proceed without consulting Marc.

I think you should evaluate every month and see what happens. You might also consider looking at the TMIN as well. Could be important!\footnote{What about multiple hypotheses in one dataset!}

Below, I have create code to evaluate all of the months at once, but you may prefer to go through each month manually and change the number from 5 to other months of the year -- but if you do you the code below, try to figure out what each line is doing. 

\begin{figure}
<<echo=TRUE, label='12MonthsTMAX', tidy=TRUE>>=
# First I create a vector of months
Months = c("January", "February", "March", "April", "May", "June",
  "July", "August", "September", "October", "November", "December")

# Create a panel so I can see all the figures at once.
par(mfrow=c(4,3), mar=c(5, 4, 3, 2) + 0.1)
TMAXresult <- NA
for (i in 1:12){
plot(TMAX~Year.num, 
  data=MonthlyTMAXMean[MonthlyTMAXMean$Month.num==i,], 
  ty='l', las=1, xlim=c(1940, 2020), main=Months[i])
Month.lm <- lm(TMAX~Year.num, data=MonthlyTMAXMean[MonthlyTMAXMean$Month.num==i,])
summary(Month.lm)
abline(coef(Month.lm), col="red")

TMAXresult <- rbind(TMAXresult, 
    cbind(Months[i], round(coef(Month.lm)[2], 4), 
      round(summary(Month.lm)$coefficients[2,4],4), 
      round(summary(Month.lm)$r.squared, 3)))
}
@
\end{figure}

\subsection{Next Steps}

\subsubsection{Analyzing Minimum Daily Temperatures}

Alternatively, it might be important to evaluate changes to the daily mininum temperatures. Following the same steps we used before but using the TMIN instead of TMAX, let's analyze the monthly average of daily minimum temperatures by following these steps: 

\begin{enumerate}

\item First, let's plot the daily minimum temperatures, and as with the daily maximum temperatures, find tons of scatter (Table \ref{fig:TMIN_trend}).

% Additional LaTeX code to add caption to figure
\begin{figure}
\label{fig:TMIN_trend}
\caption{Minimum Daily Temperatures in Bangkok, Thailand.}
<<echo = F, fig = 'true' >>=
par(las=1)
plot(TMIN ~ NewDate, data=climate_data, type="l", ylab="degrees", xlab="Date", 
     main="Minimum Daily Temperature, \n Bangkok, Thailand" )

abline(coef(lm(TMIN~NewDate, data=climate_data)), col="red", lwd=2)
@
\end{figure}

There appears to be a trend, but it's clouded with lots of variation. 

  \item We create a monthly TMIN mean for each month.

<<label='CreateTMIN'>>=
MonthlyTMINMean = aggregate(TMIN ~ Month + Year, climate_data, mean)

# Fixing the Format of Month and Year as numeric
MonthlyTMINMean$Year.num = as.numeric(MonthlyTMINMean$Year)
MonthlyTMINMean$Month.num = as.numeric(MonthlyTMINMean$Month)
head(MonthlyTMINMean)
@

\item Create a plot of the monthly average of the daily minimum temperatures. 


<<>>=
plot(MonthlyTMINMean$TMIN, ty='l')
@

There is still lots of scatter and now we can subset our data by month. 

\item Using the example above, we'll plot all 12 months at once to look for patterns (Table \ref{fig:TMIN}).

\begin{figure}[ht]
\caption{Twelve Months of Monthly Average Daily Minimum 
Temperatures, Bangkok, Thailand}
\label{fig:TMIN}
<<echo=FALSE, label='12MonthsTMIN'>>=
Months = c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December")

par(mfrow=c(4,3), mar=c(5, 4, 3, 2) + 0.1)
TMINresult <- NA
for (i in 1:12){
plot(TMIN~Year.num, data=MonthlyTMINMean[MonthlyTMINMean$Month.num==i,], ty='l', las=1, xlim=c(1940, 2020), main=Months[i])
Month.lm <- lm(TMIN~Year.num, data=MonthlyTMINMean[MonthlyTMINMean$Month.num==i,])

summary(Month.lm)

abline(coef(Month.lm), col="red")

TMINresult <- rbind(TMINresult, cbind(Months[i], round(coef(Month.lm)[2], 4), round(summary(Month.lm)$coefficients[2,4],4), round(summary(Month.lm)$r.squared, 3)))
}
@
\end{figure} 
\clearpage

\item The change in minimum temperatures seems to be even more compelling than the maximum temperatures. To compare, look at the Table \ref{tab:results} to appreciate estimated slopes and their associated null hypothesis probabilities. 

<<echo=T, results='asis', split=FALSE>>=
# Creating a dataframe to be used in a table.
Results <- data.frame(Month = TMINresult[c(2:13),1], 
    TMINSlope = TMINresult[c(2:13),2], 
    TMIN_P = as.numeric(TMINresult[c(2:13),3]), 
    TMINRsq = TMINresult[c(2:13),4], 
    TMAXSlope = TMAXresult[c(2:13),2], 
    TMAX_P = as.numeric(TMAXresult[c(2:13),3]), 
    TMAXRsq = TMAXresult[c(2:13),4])
Results$starTMAX <- ifelse(
  Results$TMAX_P <= .001, "***",
    ifelse(Results$TMAX_P <= .01, "**",
      ifelse(Results$TMAX_P <= .05, "*", "NS")))
Results$starTMIN <- ifelse(
  Results$TMIN_P <= .001, "***",
    ifelse(Results$TMIN_P <= .01, "**",
      ifelse(Results$TMIN_P <= .05, "*", "NS")))
Results$TMINslope=paste(Results$TMINSlope, Results$starTMIN)
Results$TMAXslope=paste(Results$TMAXSlope, Results$starTMAX)
colnames(Results) <- c("Month", "2", "3", "R^2", "5", "6", 
                       "R^2", "8", "9", "Slope TMIN", "Slope TMAX")

library(xtable)
print(xtable(Results[,c(1, 10, 4, 11, 7)]))
@

Based on the results above, the slopes are greatest during the dry season (starting in May) for the maximum temperatures -- but the minimum temperatures show the largest slopes (change) and peaking between January and April.  

In addition, the $r^2$ values signify the amount of the variance explained by the predictor -- in the case of TMIN, most of the values are over 20\% meaning that over 20\% of the variance is explained by time. While in March and April over time explains 50\% of the variance. 

This is very high for uncontrolled experiments. However, we should be cognizant that in many cases, especially for the maximum temperatures, it is less than 10\%. This means the the variation in temperature are not predicted by time -- thus, as a modeler, I would work very hard to capture other sources to better understand what is going on in Thailand. 

Finally, we should also be very concerned about testing 2 dozen hypotheses with our little R code. It's easy to do, but based on change alone, with a critical value of 0.05, we should expect 1 in 20 tests to give us a Type I error, a signal when one doesn't exists. Since we did 12 tests, we should expect a good chance that one or more of our tests will reject the null hypothesis incorrectly. Yikes!  
Please keep this in mind and be careful to avoid this potential problem. 

As we might expect, the a small amount of the variance is explained by the ``Month.'' Many things predict temerpature, that year is one, is quite problematic.

\item What we have not determined is the cause. So, be careful when you describe the results, cause and effect cannot be analyzed using this method.

\end{enumerate}

\subsubsection{Precipitation: Departure from Mean}

Precipiation might depend more on the departure from the mean (often referred as as normal, whatever that means!).  I think it's worth pursuing, but have found some datasets have problems. 

Precipitation is something that might increase or decrease due to climate change. So, to analyze this, we will evaluate how much precipitation has deveated from the mean, by plotting the rainfall and the mean in a time-series plot. 

Second, we need to remove the missing values and evaluate which years that are complete -- why do you think missing variables might be an issue analyzing precipiation? 

If you are missing rainy months, then the whole year should be thrown out -- but what about partial years in the drought season? We'll need to be consistent -- assuming that missing data are not zeros, we'll define complete years as over 300 days of data. 

NOTE: The missing values have not been converted to NAs!
<<>>=
climate_data$PRCP[climate_data$PRCP==-9999] <- NA

Missing <- aggregate(is.na(climate_data$PRCP), 
          list(climate_data$Month, climate_data$Year), sum)

# The aggregate command is used to create a simplified dataset. In this case
# we are creating a sum of PRCP based on each month and year.

Missing$Date = as.numeric(Missing$Group.1) + as.numeric(Missing$Group.2)/12

plot(x ~ Date, data=Missing)
@

Third, we will need to decide what level of aggredation -- monthly, yearly, etc.  Let's aggreate by month and year to get monthly totals. 

There are loads of missing values in many months. Let's cut of the months that have more than 4 missing days. 

<<>>=
TotalPPT <- aggregate(climate_data$PRCP, 
            list(climate_data$Month, climate_data$Year), sum, na.rm=T)

# Check to see what you created.

names(TotalPPT) = c("Group.1", "Group.2", "ppt")

NonMissing <- Missing[Missing$x < 5, c(1:3)]
library(dplyr)
PPT <- merge(TotalPPT, NonMissing, all.y=TRUE)
PPT$Date <- as.numeric(PPT$Group.1) + as.numeric(PPT$Group.2)/12
head(PPT)
@

First, we need a ``mean" -- The IPCC uses 1961-1990 as a norm for temperature, I don't know what is the standard for rainfall or Thailand, so we should look that up. For now, we'll use our filtered records to generate a mean.

<<>>=
PRCP_mean = mean(PPT$ppt)
@

<<>>=
plot(ppt~Date, data=PPT)
abline(h=PRCP_mean, col="blue")
@

Wow, these data look terrible -- the mean looks meaningless given the biased data set. I don't think we can do more analysis with this. But let's look at a few months and see what we can decipher.

\begin{figure}
<<echo=F>>=
names(PPT) = c("Month", "Year", "ppt", "Missing", "Date")
PPT$Month.num = as.numeric(PPT$Month)

par(mfrow=c(4,3))
for(i in 1:12){
plot(ppt~Date, data=PPT[PPT$Month.num==i,], las=1)
abline(coef(lm(ppt~Date, data=PPT[PPT$Month.num==i,])))
}
@
\end{figure}
 
%Fourth, in CA the water year starts in Oct 1. Should we follow the same convention?

<<>>=
#LosAngeles$PRCP[LosAngeles$PRCP==-9999] <- NA
#YearlySum = aggregate(PRCP ~ Year, LosAngeles, sum)
#YearlySum$YEAR = as.numeric(YearlySum$Year) 
#YearlyMean = mean(YearlySum$PRCP)
@

A yearly mean, based on the annual sum for the entire records. Not sure this is appropriate.

Figure has points of the yearly sum of rainfall and the blue line mean. The greenline is the trend and red line is a five year running average, I think!  I am still trying to understand what the code is doing.

<<>>=
#plot(PRCP~YEAR, data=YearlySum, las=1, ty="p")
#abline(h=YearlyMean, col="blue")
#YearlySum.lm = lm(PRCP~YEAR, data=YearlySum)
#abline(coef(YearlySum.lm), col="green")

#n <- 5
#k <- rep(1/n, n)
#k

#y_lag <- stats::filter(YearlySum$PRCP, k, sides=1)
#lines(YearlySum$YEAR, y_lag, col="red")
@

%The model suggests that the precipitation is declining at a rate of `r coef(YearlySum.lm)[2]` cm yr$^{-1}~$, or `r round(coef(YearlySum.lm)[2]*10, 2)` cm decade$^{-1}$.

<<>>=
#summary(YearlySum.lm)
@

\subsection{Assumptions of the Linear Regression}

Regression models, like all statistics, rely on certain assumptions. Violations of these assumptions reduces the validity of the model. If the violations are serious, then the model could be misleading or even incorrect.

TBD

%Here is a list of assumptions to produce a valid regression model:

%\begin{description}
%  \item[Homogeneity of Variance]
%  \item[something else]
%\end{description}

\subsubsection{Assumptions about $\epsilon$}

The error term should have 


\begin{description}
  \item [E(et) = 0], zero mean

  \item[E(et) = s], constant variance

  \item[E(et, Xt) = 0] , no correlation with X 

  \item[TBD] E(e , e ), no autocorrelation. t t-1

  \item[e ~ Normally distributed] (for hypothesis testing). 

\end{description}

Assumption four is especially important and most likely not to be met when using time series data.

Autocorrelation.

1. It is not uncommon for errors to “track’ themselves; that is, for the error a time t to depend in part on its value at t - m, where m is a prior time period.

\subsubsection{Model Diagnostics}

With every statistical test done, researchers validate their model in some way or anther. Often this entails the use of diagnostics, a standardize battery of procedures to check to see if the data are following the assumptions. 

In R four plots are created by default.  To see them all at the same time, we need to change the graphical parameters, using the par() function. In this case, we use \texttt{par(mfrow=c(2,2))} to create alter the graphics window expects four panels, in this case a 2 rows and two columns.

Try not to get bogged down in the code at this point. But noting this function can be handy in a number of ways to improve one's graphics. 

% Additional LaTeX code to add caption to figure
\begin{figure}
\label{fig:diagnostics}
\caption{Default diagnostic plots for a linear model in R.}
%\setkeys{Gin}{width=0.75\textwidth} % LaTeX code to read the graphic file in at 75% of its original size
% R code chunk that produces a graphic
<<echo = T, fig= 'true' >>=
par(mfrow=c(2,2))
plot(lm(TMIN ~ YEAR, data=MonthlyTMINMean[MonthlyTMINMean$MONTH==1,]))
@
\end{figure}

To determine the validity of linear model assumptions (e.g. normality or heterogeneity of variance), you have probably used statistical tests; in contrast statisticians almost exclusively look at diagnostic plots. Why?  When assumptions are violated the tests to determine violations do not perform well. So, let's see how to look at these assumptions graphically with these diagnostic plots. Linear models should have diagnostic plots that do not have any obvious structure or pattern. In this case, Figure~\ref{fig:diagnostics} should show a great deal remaining structure in the residuals. Although for today, we are not going to try to interpret these figures, but you should notice there is a ton of unaccounted structure, i.e. variance, in the model. This is due, in part, to a violation of independence; these data are serially correlated and the model does not account for that and is inappropriate because of this. It also appears that a straight-line model does not fit well and a curvilinear should be investigated.

A properly specified model is shown in 

%Figure~\ref{fig:co2_data_mlo}. In this case, the trend line has been developing using a time series analysis, which is beyond the scope of this course. Nevertheless, you want to keep this in mind during the semester because we will see a fair amount of data that looks like this.

\end{document}
