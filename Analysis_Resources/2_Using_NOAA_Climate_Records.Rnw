\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}
\title{Reading and Processing NOAA Climate Records in R}
\author{Marc Los Huertos and Isaac Medina}

\begin{document}
\maketitle

\section{Introduction}
Raw data sets often come with untidy/non-useful formats or information that must first be cleaned or processed before an accurate and useful analysis of the contents can be done. After obtaining a data set there are some preliminary steps you must follow in order to get your data file into working order for your analysis. 

\subsection{Purpose}
This document is intended as a resource and guide to help you: 
\begin{itemize}
\item upload your data file into the R environment using the Rstudio Server; and 
\item clean, organize and reformat the data to prepare it for analysis. 
\end{itemize}


\section{Preparing CSV file(s)}

\subsection{Upload CSV Files into Appropriate Rstudio Directory} 
The first step to getting your data into R is to upload it from your computer into the the Rstudio server online but to simplify our work in the future first \textbf{locate the file} in your computer and rename it using the following convention: \\ 
\begin{center}
\textbf{yourname\_region\_data}
\end{center}
where you fill in your firstname and a short descriptor for the region to which your data pertain. \\

Now follow these steps to upload the data into the Rstudio server:

  \begin{enumerate}
%  \item In Rstudio click on the folder entitled \textbf{``Climate\_Change\_Narratives"} using the \textit{Files} tab in the lower right navigation GUI (Window 4 from SOP 06)
%  \item Navigate to the folder entitled \textbf{``Data"} 
%  \item Next click on the correct folder for your class, it should correspond to the term in which you are enrolled in the EA 30 class. For example: SP17 if you're enrolled in EA 30 spring semester of 2017.
  \item Use the upload button to select a file from your computer to upload into the rstudio server
  \begin{figure}[h]
  \includegraphics[scale=0.25]{"../Admin/graphics/Upload_button"}
  \end{figure}
  \item In the popup window select \textbf{Browse} and navigate to the climate data file you downloaded from the CDO website (Step 1---Obtaining Climate Records). It should have the new name you gave it above (yourname\_region\_data)
  \item Click \textbf{Open} and then click \textbf{OK}. 
  
  \end{enumerate}

Your data file is now uploaded to the Rstudio server! All this means is that the actual file is in your workspace on the server. %This however, doesn't mean that the file has been uploaded (or ``pushed") to the repository on Github.com. If Marc has added you as a collaborator already now would be a good time to go through the process of pulling, committing and pusing with Git. Ask Marc to guide you through this in lan, and continue to the next section. 


%\subsection{Preprocessing CSV files}

%In most cases, we don't need to preprocess the csv files before uploading them. However, Mac users have been confronted with a host of problems that has something to do with how Macs format CSV files. If you have any problems with reading or uploading your csv file on a Mac speak with the instructor or TA. \footnote{I will update this when I try using a Mac for this.}


\section{Reading CSV Files into R}

\subsection{Creating a Record of your R-code}

Now that we got the file saved onto the Rstudio Server it's time to tell R to read the file so we can look at the data! This will require telling R to read the file. In order to do this we will use R commands. (remember these commands are to be typed in the R ``console") %One way to confirm this is to look at the Rstudio tab \textbf{`Environment'} in the top right window, where the file shoud be listed. If you don't see anything here it's because the file has not yet been loaded into R.

To begin, create a `R Markdown' file by navigating the menu in the following order 'File/New File/R Markdown'. Enter a title (can be changed later), enter your name as the author, and make sure you have selected html as an output option, which can also be changed later! 

I suggest you save the file and 'knit' it to see what happens, i.e. it works!  I will 'knit', which also saves the files every few minutes to make sure the code is doing what I figured it should do, i.e. not give me errors!

Except for the setup options chuck, we can begin altering the R chunks with code that will document your work AND if you have trouble, you can show the problem to the instructor for trouble shooting!

 \subsection{Finding the data path to the csv file}
In order to tell R to read the file you also have to let it know where the file is (R is funny that way). We use an R command to help us find the specific data path to the file. \\
In the console type in the following and then follow these steps: 
\begin{verbatim}
file.choose()
\end{verbatim}

\begin{itemize} 
\item press enter on your keyboard
\item in the resulting popup window navigate to the your saved data file and \textbf{double click} on it
\item Look at the console window.. under the command that you just typed you should now see a data path. The data path should look something like this: ``/home/CAMPUS/im022012/Singapore\_ClimateData.csv"  %It is the location of where your file is saved on the server. I suggest you truncate it to the path that after the home directory, i.e. ''Data/test\_data/Singapore\_ClimateData.csv''.

\item \textbf{Copy} the data path (including the quotation marks)
\end{itemize}

I suggest we create an object with the path, for example, often write out something like this:

\begin{verbatim}
climate_data.csv = ``your file path''
\end{verbatim}

\noindent where I paste in the results of the file choose into the quoted section. Do not use write ``your file path''! That is the intent, not the exact content!


\subsection{Telling R to Read the Data File}

Now that we know where the data file is we will tell R to read it! In the console type in the following command and follow the next steps: 
\begin{verbatim}
read.csv()
\end{verbatim}

\begin{itemize}
\item This time before you press enter on the R command \textbf{Paste} in the data path you copied inbetween the parenthesis of the R command (be sure to include the parenthesis). As described above, I recommend using the path object we created, i.e.

\begin{verbatim}
read.csv(climate_data.csv)
\end{verbatim}

\item press enter
\item If your command worked you should see a bunch of text in your console window now. This means R read your file!
\end{itemize}

\subsection{Creating a Data Object}
So we made R read our file, however cool this might be, it is still not useful to us. In order to manipulate and do things with the data we need to tell R not only to read the file but also to store it in an object (such as a data frame) that we can manipulate. Follow these steps to tell R to read the file and store it in an object: \\

This time we will use the following R command\\
\begin{verbatim}
climate_data <- read.csv(``the/file/path'')
\end{verbatim}

\textbf{But be sure to use your actual data path!}

What this command is essentially doing is it's telling R to read the file and store it in an object called climate\_data ... if we really wanted to we could have actually named the object something else but climate\_data will suffice for now. \\ 

In R usually ``no news is good news" after you hit enter on a command. In other words, if you don't get red colored text describing an error, the program did something -- but now we need to figure out if it did something useful! Luckily there is a way to check if your command was successful in creating your object. All you have to do is check the \textbf{Environment} tab (in window 2). You should see your data object: climate\_data listed there. 


\section{Confirming the Proper Reading of the CSV file}
<<echo=FALSE, message=FALSE, results='hide'>>=
climate_data.csv = "/home/CAMPUS/mwl04747/github/Climate_Change_Narratives/Data/MLH/test_data/Singapore_ClimateData.csv" 
  
climate_data<- read.csv(climate_data.csv)
@

So we see that the data file is actually an object in R now. But what does it actually look like and what's in it? 
Since R was made to handle large data sets it tries not to overwhelm you with all the data at once but luckily the developers made a few commands to peak at the data to see if everthing is in order. We will try some of these commands now to look at few observations then we'll evaluate the structure of the dataframe and go on to finally plot the data!

\subsubsection{Viewing the 1st 6 Observations}
The following command allows you to view the first six observations of data in the data frame you created.
\begin{verbatim}
head(climate_data)
\end{verbatim}
Type it into the console and hit enter! You should get something that looks like the following

<<tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=65)>>=
head(climate_data)
@
You can see how the data is formatted into columns and rows like in Excel. 

\subsection{Evaluating the structure of the object}

Okay, so you now have created a data frame and taken a peak at it. But what if you want more information about it? Such as how many columns (a.k.a. variables) are in it. Or how many rows (a.k.a. observations) are in it. You can use the following command: \\
\begin{verbatim}
str(climate_data)
\end{verbatim} 
This function allow you to peer into the structure of the data frame giving output which describes how many variables they're are, how many observations of each as well as other useful information. Below you can see an example of the command. Checking the structure of the data frame is a another way to ensure that the data have been imported in a way that you expect.
<<tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=65)>>=
str(climate_data)
@
\subsection{Confirming the Column Names}
In R it isn't uncommon to want to manipulate a specific column of data within your data frame. Therefore it is useful to know what the names of the columns in your data frame are. The following command can be used to make R give you the names of the columns in your data frame. 

<<>>=
names(climate_data)
@

A data frame is essentially a set of ``vectors". Which themselves are like lists of numbers or text. You can think of each column as one of the vectors inside your data frame. If you want to access the data in one of the columns specifically, you can use the following command syntax:

\begin{verbatim}
nameofdataframe$columnname
\end{verbatim}

so for example, you can type in:

\begin{verbatim}
climate_data$TMAX
\end{verbatim}

And R will spit out the data in just that column. 

\section{Checking for Missing Data}

%Now we will check the data by plotting it(Figure~\ref{fig:plotmissing}). 

%\begin{figure}
%<<echo=T, fig=TRUE>>=
%plot(TMAX~DATE, climate_data)
%@
%\caption{These data are plotted with DATE on the x-asis and TMAX on the y-asis. These data have a fair amount of missing data (-9999). Not all the datasets will report missing data in this way. In some cases, they are just missing.}
%\label{fig:plotmissing}
%\end{figure}

In some cases, we might some rather odd low temperature values in the plot. Missing data used to be documented as -9999. Obviously, this will throw our means off!

Let's check if we have these using the \texttt{min()} function. This function works when there are no missing data, otherwise it will just report NA. 

<<>>=
min(climate_data$TMAX)
@

To get the function to cooperate if there are missing values, add 'na.rm=T' to the function:

<<>>=
min(climate_data$TMAX, na.rm=T)
@

\subsection{Re-assigning Missing Values to NAs}

In some of the stations, missing values were coded as  -9999? These are used for missing data. Historically, computers didn't have a lot of options for mixing numbers and letters in a variable type while R has some built in flexibility for this. So, to avoid leaving values blank (with all the ambiguous interpretations), the value -9999 is used to sybmolize missing values, since the number is unrealistic in the real world!

If your data do not have missing data, skip these steps!

%Obviously, if we averaged the temperature with these values, we'd get a pretty inaccurate number (e.g. \Sexpr{round(mean(climate\_data$TMAX), 0)} versus \Sexpr{round(mean(climate\_data$TMAX[climate\_data$TMAX>-9999]), 0)}. Thus, we need to remove them!  

If your station used -9999 for missing values, we will replace these with NA, which R uses specifically to avoid accidently averaging arbitrary values that are representing missing values. 

How do we do this?
<<>>=
climate_data$TMAX[climate_data$TMAX==-9999] = NA
climate_data$TMIN[climate_data$TMIN==-9999] = NA
@

%Okay, now we'll check again, but let's plot a just a few years, let's say five years (356 days * 5 years = 1825) or 1825 observations. I am using this to check my dataframe -- based on the number of observations that I have, but your data set will vary and you may find this selection of rows to be unhelpful.\footnote{IMPORTANT: when we plot with a type (ty) as a line, we define that as the letter 'l' not the number '1'!}

%<<>>=
%plot(TMAX~DATE, climate_data[1:1835,], ty='l')
%@

%In the case of the Singapore data we have a new problem. What's wrong? It appears we have gaps in the data -- but we already removed our missing data, why are these big jumps in the data.

As it turns out stations might have one of several types of date formats -- and each of them are problematic. The first thing to note is that they are factors instead of begin formatted as a Date. You can check this with the following function:

<<>>=
str(climate_data)
@

In this case, we find that Date is a factor, which means that we will not be able to determine a trend because R has not recognized the data as a continuous variable. Look carefully at the structure, i.e. str() of the data frame -- note the format that the date is in -- usually a factor, which is not helpful!  

Factors in R are thought of as unique observations of character or numeric strings. But R does not understand the numeric and ordered nature of dates, e.g. 12 months, some months with 28, 29 (leap year), 30, and 31 days. When you think about it, dates are very strange. 

So, we need to convert these to a date format that R understands. This is the hardest part of this process. Pay attention to the date formats!  If you assume a particular format that is wrong, it can lead to a reasonably frustrating process. Note: I have missed details from the stage below and subsequently loss of valuable time. 

\subsection{Determine and Convert Date Format}

To create a new format, we have to complete a few steps. Unfortunately, date formats are one of the more obtuse aspects of R, but if you follow along, you should have success, even if you have no clue what you did. 

First, we convert the date to a string of character values. Next, we'll convert the strings to a data format. But we need to determine which type of data format we have from the following three choices then we can make the conversion:

\begin{description}
  \item[MM/DD/YYYY] This is a standard data for many US stations. To convert the dates, we first create a vector of just the dates. Then, we can reformat the dates using the syntax below:
  
\begin{verbatim}
strDates <- as.character(climate_data$DATE)

climate_data$NewDate <- as.Date(strDates, "%m/%d/%Y")
\end{verbatim}

Caution: Be sure the dates were actually converted correctly, using the \texttt{head()} or \texttt{str()} functions.


\item[MM-DD-YYYY] Sometimes, the dates are formatted by month-day-year, where we can use the code below to reformat the dates:

\begin{verbatim}
strDates <- as.character(climate_data$DATE)

climate_data$NewDate <- as.Date(strDates, "%m-%d-%Y")
\end{verbatim}

\item[YYYY/MM/DD] Sometimes, the dates are formatted by year/month/day, where we can use the code below to reformat the dates:

  
\begin{verbatim}
strDates <- as.character(climate_data$DATE)

climate_data$NewDate <- as.Date(strDates, "%Y/%m/%d")
\end{verbatim}

\item[YYYY-MM-DD] Sometimes, the dates are formatted by year-month-dayy, where we can use the code below to reformat the dates:

  
\begin{verbatim}
strDates <- as.character(climate_data$DATE)

climate_data$NewDate <- as.Date(strDates, "%Y-%m-%d")
\end{verbatim}

\item[YYYYMMDD] In certain cases, the dates are formatted as a long number starting with year. Although these dates sort correctly, they create big gaps at the new year: Let's say that the data have a year change between 1913 and 1914. The date format in the NOAA data are YYYYMMDD, or year, month, and day with 4, 2, 2 digits, respectively. Thus, the last day of 1913 is 19131231 or Dec, 31, 1913. The next day is January 1st or 19140101. But when you plot these on the x-axis, the order of the values should be $19131231 \rightarrow 19131232 \rightarrow 19131233 \rightarrow 19131234$, etc but there is no 32nd, 33rd or 34th of December. Instead the dates go from  $19131231 \rightarrow 19140101$. We have lots of numbers that are skipped, but no coded as missing, but missing all the same. So, now we need to convert our dates to something more sensible. In R, that means creating a variable with a format that expects dates, thus doesn't plot numbers that are impossible dates!
  
<<>>=
strDates <- as.character(climate_data$DATE)

climate_data$NewDate <- as.Date(strDates, format = "%Y%m%d") 
@


\item[2-digit Years] Is a total mess. Try the instructions below, but also reach out to Marc for help, because every data set seems to have some unique issues. 

First, we need to appreciate that we might have two centuries of data the the exact date!  So, we use the order of the data to assign the century and figure out where it changes after 12/31/99. 

In the case of a location in China, we actually have a format change in the datset too! 
<<>>=
china = read.csv("/home/CAMPUS/mwl04747/github/Climate_Change_Narratives/Data/MLH/Data-China.csv")
str(china)
@

Scroll to observation 5841 to see part of the problem!
<<>>=
china[5840:5845,]
@

So, we'll have to convert the first part (1:5841), which does have a 4-year format, different than the rest, which has 2-year format. 

<<>>=
strDates <- as.character(china$DATE)
china$NewDate = NA

china$NewDate[1:5841] <-format(as.Date(china$DATE[1:5841]), "%Y-%m-%d")

# I don't know why I can't the code "%m/%d/%19y" to work... so I had to hack something lame...

strDates <- as.Date(china$DATE[5842:41964], format = "%m/%d/%y")-365*100-24


china$NewDate[5842:41964] <- strDates
  
as.Date(strDates, format="%Y-%m-%d")

china$NewDate[41965:49673] <- format(as.Date(china$DATE[41965:49673], "%m/%d/%y"))

str(china)
@



\item[Mixed Formats]... see Valentina's data!

Doesn't seem to work, I had to brut force it... will work on this next summer!


<<tidy=TRUE>>=
strDates <- as.character(climate_data$DATE)
head(strDates)

climate_data$NewDate <- as.Date(strDates, "%Y%m%d", 
    tryFormats = c("%Y-%m-%d", "%Y/%m/%d"), 
    optional = FALSE)


climate_data$NewDate <- as.Date(strDates, "%Y%m%d", 
    tryFormats = c("%Y-%m-%d", "%m/%d/%y"), 
    optional = FALSE)
@

\end{description}



\subsection{Checking the New Dates}

To check the NewDate variable, I like to make sure I can get a plot that makes sense. In the code below I have subsetted the data for the first 1835 observations -- however, you may not have that many, so you'll get an error. Remove the code with the square brackets (including the square brackets) and that should work. 

<<>>=
plot(TMAX~NewDate, climate_data[1:1835,], ty='l')
@


With these data, you can see we have a very strange pattern -- lots of missing data. At this point, I might need to find a better dataset -- however, because I truncated the data, the more recent data might be fine. This is part of the scientific process -- deciding on what qualifies as high quality data!
%\subsection{Subset Sites}

%unique(import$STATION_NAME)
%Let's choose the FAIRPLEX NY US because the record is longer than the airport.

%LosAngeles <- subset(import, STATION_NAME=="LOS ANGELES DOWNTOWN USC CA US", select=c(STATION, STATION_NAME, DATE, NewDate, TMIN, TMAX, PRCP))


%plot(TMAX~NewDate, LosAngeles, ty='l')


<<echo = F, results = 'hide' >>=
#maunaloa$average
@

%to dump the average CO$_2$ concentrations readings onto your screen as a vector. You should see some ~627 observations, depending on how recent the data have been uploaded. So, the dollar symbol is used to drill into the data frame vectors.  And when you look at the \texttt{str()} function again, you will see these dollar signs again.

\section{Preparing Records for Analysis}

Our next task is to create a template to automatically run our code, so we can regenerate the pre-processed data within a Rmd file, which will then produce a figure. 

\section{Summarizing what Our Code is Doing}

%Start a new file -- File/New/Rmarkdown. Enter a draft title and make sure the author name is correct. Save the file in the your student folder. 

%Knit to make sure it works. 

%Now you will begin creating your blog by getting the commands you created before into an R block so that your data is available for the kniting process.

Thus, far, we have already completed the following\ldots

\begin{enumerate}
  \item Find the path for the csv file (\texttt{file.choose()}).
  \item Read the file into R (\texttt{read.csv(filepathfilename.csv})).
  \item Replace missing values with NA.
  \item Fix the date formats.
\end{enumerate}

%But now we need to paste this into the Blog Rmd (R markdown) file. After you put your code into the .Rmd file, make sure it knits. If not , please stage, commmit, and push to github and send a note to Marc or Char for help. 

\noindent If you have been successful with these steps, our nextg action items include the following: 

\begin{enumerate}
  \item Create a figure of Tmax versus time (\texttt{plot()}).
  \item Overlay the best fit line (\texttt{abline()}, \texttt{coef()}, and \texttt{lm()}).
  \item Go to "Step 3: Evaluating Monthly Trends using CHCN-Daily" to learn about how to analyze data.
\end{enumerate}

Note, I have not outlined how to do this here, but it is outlined in the next guide. 

\end{document}